{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18106,
     "status": "ok",
     "timestamp": 1651387091841,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "ljRApNerjGld",
    "outputId": "af213406-6b14-4d40-a211-79e7a1a5e319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.8 MB 4.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 144 kB 11.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 181 kB 39.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 63 kB 588 kB/s \n",
      "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb -U -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3w3SAJwaeyS"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import wandb\n",
    "import gensim\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "import multiprocessing\n",
    "import gc\n",
    "from gensim.models import KeyedVectors\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR, StepLR\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1651387097212,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "L-OQLaW6wfNI",
    "outputId": "573d8011-b459-46e8-ed0b-ca2a57b1f25a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13945,
     "status": "ok",
     "timestamp": 1651387111149,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "ciRj4Uz8cnND",
    "outputId": "77ef1292-8c5a-4f92-b8bb-44990017cb92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOvpWOHfco5D"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/Custom_Functions/\")\n",
    "import custom_functions as cf\n",
    "import custom_preprocessor as cp\n",
    "import plot_learning_curve as plc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DJgtvD3cuLT"
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"/content/drive/MyDrive/Datasets_Models/Datasets/\")\n",
    "model_path = Path(\"/content/drive/MyDrive/Datasets_Models/Models/\")\n",
    "embedding_path = Path(\"/content/drive/MyDrive/Datasets_Models/Word_Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoWoKEs3c5LL"
   },
   "outputs": [],
   "source": [
    "X_cleaned_comp = data_path/\"df_stackoverflow_multilabel_X_cleaned.joblib\"\n",
    "y_cleaned_comp = data_path/\"df_stackoverflow_multilabel_y_cleaned.joblib\"\n",
    "X = joblib.load(X_cleaned_comp)\n",
    "y = joblib.load(y_cleaned_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1651387115870,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "407CGhHDdA6V",
    "outputId": "95f4e38b-0d72-421a-f248-cef0ebef47ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asp query string dropdowni webpage follow control relevance    dropdownlist value hyperlink redirect page call   page cancel button redirect user menu page like user click hyperlink edit page index dropdownlist preserve query string page follow aspx code sure proceed < asp hyperlink      id=\"lnkedit      navigateurl=\\'<% + eval(\"userid + sure > < /asp hyperlink >   < asp dropdownlist      id=\"mydropdown      < asp listitems/ > < /asp dropdownlist >   edit clarify m navigateurl query string eval determine user id', 'run javascript code server java code?i want run javascript code server want manipulate result return javascript inside java code', 'linq sql throw exception row find changedhi linq sql get error row find change update table help linq query show error unable figure problem work get permanent solution fix problem twtmob_campainincomedetails_tb incomedetail = datacontext.twtmob_campainincomedetails_tb single(twtincome = > = = tempincome                  decimal temppayout = decimal parse(lblpertweet text                  decimal temptotal = temppayout + tempmoneyearne                  = convert tostring(temptotal                  = temptweet + 1                  = lblbonus text                  = tempbudurl                  datacontext submitchange    twtmob_user_tb twtuserdetail = datacontext.twtmob_user_tbs single(twtdetail = > = = tempuserid                       float temppayout = float parse(lblpertweet text              float tempoutstandingtotal = temppayout+tempoutstanding              = tempoutstandingtotal              datacontext submitchange           ']\n",
      "[['0', '9'], ['1', '3'], ['0', '9']]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:3])\n",
    "print(y[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GW7eu05XdDUs"
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 2476,
     "status": "ok",
     "timestamp": 1651387118459,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "Uwj3qMePdPjV",
    "outputId": "2aeac176-fc35-41e4-bb86-30a8fa1d86ed"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTV9-jwJdMka"
   },
   "outputs": [],
   "source": [
    "# X_train, X_valid, X_test, y_train, y_valid, y_test = cf.train_valid_test_split(X, y, 0.6, 0.2, 0.2, stratify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foiP7zReklRt"
   },
   "outputs": [],
   "source": [
    "X_gensim_cleaned = list(map(gensim.utils.simple_preprocess, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EohJNaBks8J"
   },
   "outputs": [],
   "source": [
    "X_gensim_cleaned_X = []\n",
    "for lst in X_gensim_cleaned:\n",
    "    X_gensim_cleaned_X.append(\" \".join(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1651387124676,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "8kK8eHs2kvde",
    "outputId": "738971d9-5497-44a0-c4a4-cd543a111c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 28456, X_valid: 9485, X_test: 9486, y_train: 28456, y_valid: 9485, y_test: 9486\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = cf.train_valid_test_split(X_gensim_cleaned_X, y, 0.6, 0.2, 0.2, stratify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap_G5fyZlDTf"
   },
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJI4qfGqk7Pe"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.array(X)\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        text = self.X[idx]\n",
    "        labels = self.y[idx]\n",
    "        sample = (text, labels)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rG5osO9VlGe5"
   },
   "outputs": [],
   "source": [
    "trainset = CustomDataset(X_train,y_train)\n",
    "validset = CustomDataset(X_valid,y_valid)\n",
    "testset = CustomDataset(X_test,y_test)\n",
    "# trainset_gen = CustomDataset(X_train1,y_train1)\n",
    "# validset_gen = CustomDataset(X_valid1,y_valid1)\n",
    "# testset_gen = CustomDataset(X_test1,y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1651387127162,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "NsIlENS-mnsF",
    "outputId": "c398a6f6-20d9-4cb1-8090-645a635df4c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['code multiple button navigation java activity wonder optimize create activity multiple listener create multiple java file button onclick listener question try create multiple listener java button work syntax multiple listener java file update code issue matter button click lead page package import activity import context import intent import bundle import button import view import view onclicklistener public class activity extend activity button button button button button button button button button button button button public void oncreate bundle super oncreate setcontentview layout fineline public void final context context button button findviewbyid id autobody button new onclicklistener public void onclick view arg intent intent new intent context startactivity intent button button findviewbyid id glas button new onclicklistener override public void onclick view arg intent intent new intent context startactivity intent button button findviewbyid id wheels button new onclicklistener override public void onclick view arg intent intent new intent context startactivity intent button button findviewbyid id speedy button new onclicklistener override public void onclick view arg intent intent new intent context startactivity intent button button findviewbyid id sevan button new onclicklistener override public void onclick view arg intent intent new intent context startactivity intent button button findviewbyid id towe button new onclicklistener override public void onclick view arg intent intent new intent context startactivity intent package import activity import bundle import button public class activity extend activity button button public void oncreate bundle super oncreate setcontentview layout autobody button button public void oncreate bundle super oncreate setcontentview layout glass button button public void oncreate bundle super oncreate setcontentview layout wheel button button public void oncreate bundle super oncreate setcontentview layout speedy button button public void oncreate bundle super oncreate setcontentview layout sevan button button public void oncreate bundle super oncreate setcontentview layout towe'],\n",
       "       dtype='<U20742'), array([[0, 1, 0, 0, 1, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.__getitem__([11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wVLN8LmudX0"
   },
   "outputs": [],
   "source": [
    "# trainset_gen.__getitem__([11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwTbZe9GmtX6"
   },
   "outputs": [],
   "source": [
    "def create_vocab(dataset, min_freq):\n",
    "  counter = Counter()\n",
    "  for (text, _) in dataset:\n",
    "    counter.update(str(text).split())\n",
    "  my_vocab = vocab(counter, min_freq=min_freq)\n",
    "  my_vocab.insert_token('<unk>', 0)\n",
    "  my_vocab.set_default_index(0)\n",
    "  return my_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1AIO-b4nEQz"
   },
   "outputs": [],
   "source": [
    "trainset_vocab = create_vocab(trainset, min_freq = 2)\n",
    "# trainset_gen_vocab = create_vocab(trainset_gen, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1651387129715,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "heu_NdW4nt5h",
    "outputId": "0d9eca2d-e98a-49be-acf6-6bf64795109a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51304\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset_vocab))\n",
    "# print(len(trainset_gen_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1651387129715,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "Top8XfLnn0qB",
    "outputId": "a403b715-1548-408a-bb17-9d02375c2c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'rearrange', 'order', 'list', 'web', 'page', 'base', 'current', 'day', 'work', 'site', 'restaurant', 'daily', 'specials', 'take', 'vertical', 'space', 'li', 'consist', 'week']\n"
     ]
    }
   ],
   "source": [
    "print(trainset_vocab.get_itos()[0:20])\n",
    "# print(trainset_gen_vocab.get_itos()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOVOARyjovXW"
   },
   "outputs": [],
   "source": [
    "# print_itos = list(set(trainset_vocab.get_itos()) - set(trainset_gen_vocab.get_itos()))\n",
    "# print(print_itos[:10])\n",
    "# print_itos = list(set(trainset_vocab.get_itos()) & set(trainset_gen_vocab.get_itos()))\n",
    "# print(print_itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1651387129824,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "qEUb8bNHqoiv",
    "outputId": "05977f93-6b12-4785-f5d0-4a2dc2991630"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13276"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_vocab['dotnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-7sHu6Sr481"
   },
   "outputs": [],
   "source": [
    "# trainset_gen_vocab['dotnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWxoajpnr-7w"
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [trainset_vocab[token] for token in str(x).split()]\n",
    "# text_pipeline_gen = lambda x: [trainset_gen_vocab[token] for token in str(x).split()]\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8ygGeEFuqzD"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_text, _label) in batch:\n",
    "         label_list.append(torch.tensor(_label).float())\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.stack(label_list)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return text_list, label_list, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgnKMijrv_lU"
   },
   "outputs": [],
   "source": [
    "# def collate_batch_gen(batch):\n",
    "#     label_list, text_list, offsets = [], [], [0]\n",
    "#     for (_text, _label) in batch:\n",
    "#          label_list.append(label_pipeline(_label))\n",
    "#          processed_text = torch.tensor(text_pipeline_gen(_text), dtype=torch.int64)\n",
    "#          text_list.append(processed_text)\n",
    "#          offsets.append(processed_text.size(0))\n",
    "#     label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "#     offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "#     text_list = torch.cat(text_list)\n",
    "#     return text_list, label_list, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbqfoP3JwPGK"
   },
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "check_loader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            collate_fn=collate_batch,\n",
    "                                            num_workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1651387129948,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "YVctuoIMyuZP",
    "outputId": "e5127d5e-60a8-4bf1-93a1-d861fede0022"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0.]]) tensor([  114,   701,    92,    79, 13383,    29,   114,   701,    92,   375,\n",
      "          506,  3006,    85,     9,  1555,  3006,   140, 13384,   997,  3106,\n",
      "          141,   118,   375,   491, 13384,   437,  1041,  3006,    85,   133,\n",
      "        13384,   131,   942, 13384,  1075,   131,   942, 13384,    92,   701,\n",
      "          108,  3925,   458,  1105,   559,   771,  1226,   621,   857,   466,\n",
      "          140,  2758,   140,   258,   502,  2758,   140,   113,   725,   467,\n",
      "           22,  1226,   771,   967,   120,   467,    70,   108,   356,   140,\n",
      "          137,   133,   279,  1150]) tensor([ 0, 40])\n"
     ]
    }
   ],
   "source": [
    "for text, label, offsets in check_loader:\n",
    "  print(label, text, offsets)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vHj4FkU_-Su"
   },
   "outputs": [],
   "source": [
    "# Fix seed value\n",
    "SEED = 2345\n",
    "random.seed(SEED)\n",
    "\n",
    "# We will be using 500 observations for text and 100 for valid daatset\n",
    "# We do not need valid for overfitting, it will help to check errors in code\n",
    "train_sample_size = 500\n",
    "valid_sample_size = 100\n",
    "\n",
    "# Getting n random indices\n",
    "train_subset_indices = random.sample(range(0, len(trainset)), train_sample_size)\n",
    "valid_subset_indices = random.sample(range(0, len(validset)), valid_sample_size)\n",
    "\n",
    "# Getting subset of dataset\n",
    "train_subset = torch.utils.data.Subset(trainset, train_subset_indices)\n",
    "valid_subset = torch.utils.data.Subset(validset, valid_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMznW_2Dy5s_"
   },
   "outputs": [],
   "source": [
    "pretrained_vectors1000 = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets_Models/Word_Embeddings/GoogleNews-vectors-negative300.bin', binary=True, limit=1000)\n",
    "pretrained_vectors2000000 = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets_Models/Word_Embeddings/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1651387210498,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "V-9iC0_k4xUT",
    "outputId": "7162ce29-595e-4e31-e1f0-8df33ec7b6cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = pretrained_vectors1000.wv.get_vector('office')\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLPDix9543Y5"
   },
   "outputs": [],
   "source": [
    "embedding_dim =300\n",
    "test_weights = np.zeros((2, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrbyynYI9nB5"
   },
   "outputs": [],
   "source": [
    "test_weights[0] = pretrained_vectors1000.wv.get_vector('office')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbCk86xI9uHK"
   },
   "outputs": [],
   "source": [
    "test_weights[1] =  np.random.normal(size=(embedding_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFgSq71n9xty"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "pretrained_weights = np.zeros((len(trainset_vocab), embedding_dim))\n",
    "words_found = 0\n",
    "words_not_found = 0\n",
    "\n",
    "for i, word in enumerate(trainset_vocab.get_itos()):\n",
    "    try: \n",
    "        pretrained_weights[i] = pretrained_vectors2000000.wv.get_vector(word)\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        words_not_found  += 1\n",
    "        pretrained_weights[i] = np.random.normal(size=(embedding_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1651387211834,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "VTy4OTnw9_5-",
    "outputId": "a3d2a9e4-9757-4af9-af35-c70e06403b8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12037"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1651387211835,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "fgtJvYQr-Gzm",
    "outputId": "b8f7209a-cf75-4be3-8e00-fa9e0cc34bd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39267"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dtUwJ4M-ImE"
   },
   "outputs": [],
   "source": [
    "class MLPCustom(nn.Module):\n",
    "  def __init__(self, vocab_size, hidden_sizes_list, dprobs_list, batchnorm_binary, output_dim, non_linearity, pretrained_weights):\n",
    "\n",
    "    \n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    \n",
    "\n",
    "    self.hidden_sizes_list = hidden_sizes_list # hidden_sizes = [emb_dim, hidden_dim1, hidden_dim2, .....output_dim] # n + 1 elements\n",
    "    self.dprobs_list = dprobs_list # dpropb =[prob1, prob2....probn] # n elements\n",
    "    self.batchnorm_binary  = batchnorm_binary  # True or False\n",
    "    self.output_dim = output_dim\n",
    "    self.non_linearity = non_linearity\n",
    "\n",
    "    self.pretrained_weights = pretrained_weights\n",
    "    # self.task = task\n",
    "    \n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    # embedding_layer\n",
    "    \n",
    "    # if self.task == 2:\n",
    "    #    self.embedding_layer = nn.EmbeddingBag(vocab_size, self.hidden_sizes_list[0])\n",
    "\n",
    "    # Task 5\n",
    "    # if self.task == 5:\n",
    "    # self.embedding_layer = nn.EmbeddingBag(vocab_size, self.hidden_sizes_list[0]).from_pretrained(pretrained_weights,\n",
    "    #                                                                            freeze = True)\n",
    "       \n",
    "    # Task 6\n",
    "    # if self.task == 6:\n",
    "    self.embedding_layer = nn.EmbeddingBag(vocab_size, self.hidden_sizes_list[0]).from_pretrained(pretrained_weights,\n",
    "                                                                               freeze = False)\n",
    "\n",
    "    # hidden layers\n",
    "    self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "    # dropout layers\n",
    "    self.dropout_layers = nn.ModuleList()\n",
    "\n",
    "    # batchnorm layers\n",
    "    self.batchnorm_layers = nn.ModuleList()\n",
    "\n",
    "    for k in range(len(self.hidden_sizes_list)-1):\n",
    "      self.hidden_layers.append(nn.Linear(self.hidden_sizes_list[k], self.hidden_sizes_list[k+1]))\n",
    "      self.dropout_layers.append(nn.Dropout(p=self.dprobs_list[k]))\n",
    "\n",
    "      if self.batchnorm_binary:\n",
    "        self.batchnorm_layers.append(nn.BatchNorm1d(self.hidden_sizes_list[k+1], momentum=0.9))\n",
    "\n",
    "    self.output_layer = nn.Linear(self.hidden_sizes_list[-1], self.output_dim)\n",
    "\n",
    "   \n",
    "\n",
    "  def forward(self, input, offsets):\n",
    "    x = self.embedding_layer(input, offsets)\n",
    "    for  k in range(len(self.hidden_sizes_list)-1):\n",
    "      x =  self.non_linearity(self.hidden_layers[k](x))\n",
    "      if self.batchnorm_binary:\n",
    "        x = self.batchnorm_layers[k](x)\n",
    "      x= self.dropout_layers[k](x)\n",
    "\n",
    "    x = self.output_layer(x)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "763VdafI-i0o"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, loss_function, log_batch, log_interval, grad_clipping, max_norm):\n",
    "\n",
    "  \"\"\" \n",
    "  Function for training the model in each epoch\n",
    "  Input: iterator for train dataset, initial weights and bias, epochs, learning rate.\n",
    "  Output: final weights, bias, train loss, train accuracy\n",
    "  \"\"\"\n",
    "  # initilalize variables as global\n",
    "  # these counts will be updated every epoch\n",
    "  global example_ct_train\n",
    "  global batch_ct_train\n",
    "  \n",
    "\n",
    "  # Training Loop loop\n",
    "  # Initialize train_loss at the he start of the epoch\n",
    "  running_train_loss = 0\n",
    "\n",
    "  #running_train_correct = 0\n",
    "  \n",
    "  # put the model in training mode\n",
    "  model.train()\n",
    "\n",
    "  # Iterate on batches from the dataset using train_loader\n",
    "  for text, targets, offsets in train_loader:\n",
    "    \n",
    "    # move inputs and outputs to GPUs\n",
    "    text = text.to(device)\n",
    "    targets = targets.to(device)\n",
    "    offsets = offsets.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(text, offsets)\n",
    "    loss = loss_function(output, targets)\n",
    "    \n",
    "    # set gradients to zero \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Correct prediction\n",
    "    #y_pred = torch.argmax(output, dim = 1)\n",
    "    #correct = torch.sum(y_pred == targets)\n",
    "\n",
    "    example_ct_train +=  len(targets)\n",
    "    batch_ct_train += 1\n",
    "\n",
    "    # Gradient Clipping\n",
    "    if grad_clipping:\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm, norm_type=2)\n",
    "\n",
    "    # Update parameters using their gradient\n",
    "    optimizer.step()\n",
    "\n",
    "    # Add train loss of a batch \n",
    "    running_train_loss += loss.item()\n",
    "\n",
    "    # Add Corect counts of a batch\n",
    "    #running_train_correct += correct\n",
    "\n",
    "    # log batch loss and accuracy\n",
    "    if log_batch:\n",
    "      if ((batch_ct_train + 1) % log_interval) == 0:\n",
    "        wandb.log({f\"Train Batch Loss  :\": loss})\n",
    "        #wandb.log({f\"Train Batch Acc :\": correct/len(targets)})\n",
    "\n",
    "  \n",
    "  # Calculate mean train loss for the whole dataset for a particular epoch\n",
    "  train_loss = running_train_loss/len(train_loader)\n",
    "\n",
    "\n",
    "  # Calculate accuracy for the whole dataset for a particular epoch\n",
    "  #train_acc = running_train_correct/len(train_loader.dataset)\n",
    "\n",
    "  return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nl6FDWP9-p8U"
   },
   "outputs": [],
   "source": [
    "def valid(loader, model, optimizer, loss_function, log_batch, log_interval):\n",
    "\n",
    "  \"\"\" \n",
    "  Function for training the model and plotting the graph for train & valid loss vs epoch.\n",
    "  Input: iterator for train dataset, initial weights and bias, epochs, learning rate, batch size.\n",
    "  Output: final weights, bias and train loss and valid loss for each epoch.\n",
    "  \"\"\"\n",
    "\n",
    "  # initilalize variables as global\n",
    "  # these counts will be updated every epoch\n",
    "  global example_ct_valid\n",
    "  global batch_ct_valid\n",
    "\n",
    "  # Validation loop\n",
    "  # Initialize train_loss at the he strat of the epoch\n",
    "  running_valid_loss = 0\n",
    "  #running_valid_correct = 0\n",
    "  \n",
    "  # put the model in evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for text, targets, offsets in loader:\n",
    "      \n",
    "      # move inputs and outputs to GPUs\n",
    "      text = text.to(device)\n",
    "      targets = targets.to(device)\n",
    "      offsets = offsets.to(device)\n",
    "      \n",
    "      # Forward pass\n",
    "      output = model(text, offsets)\n",
    "      loss = loss_function(output,targets)\n",
    "\n",
    "      # Correct Predictions\n",
    "      #y_pred = torch.argmax(output, dim = 1)\n",
    "      #correct = torch.sum(y_pred == targets)\n",
    "\n",
    "      # count of images and batches\n",
    "      example_ct_valid +=  len(targets)\n",
    "      batch_ct_valid += 1\n",
    "\n",
    "      # Add valid loss of a batch \n",
    "      running_valid_loss += loss.item()\n",
    "\n",
    "      # Add correct count for each batch\n",
    "      #running_valid_correct += correct\n",
    "\n",
    "      # log batch loss and accuracy\n",
    "      if log_batch:\n",
    "        if ((batch_ct_valid + 1) % log_interval) == 0:\n",
    "          wandb.log({f\"Valid Batch Loss  :\": loss})\n",
    "          #wandb.log({f\"Valid Batch Accuracy :\": correct/len(targets)})\n",
    "\n",
    "\n",
    "    # Calculate mean valid loss for the whole dataset for a particular epoch\n",
    "    valid_loss = running_valid_loss/len(valid_loader)\n",
    "\n",
    "    # scheduler step for learning rate on Plateau\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    # scheduler step for stepLR\n",
    "    # scheduler.step()\n",
    "\n",
    "    # Calculate accuracy for the whole dataset for a particular epoch\n",
    "    #valid_acc = running_valid_correct/len(valid_loader.dataset)\n",
    "    \n",
    "  return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3o5bncjW-ru_"
   },
   "outputs": [],
   "source": [
    "def train_loop(train_loader, valid_loader, model, loss_function, optimizer, epochs, device, \n",
    "               patience, early_stopping,\n",
    "               file_model, save_best_model):\n",
    "\n",
    "  '''\n",
    "  model: specify your model for training\n",
    "  criterion: loss function \n",
    "  optimizer: optimizer like SGD , ADAM etc.\n",
    "  train loader: function to carete batches for training data\n",
    "  valid loader : function to create batches for valid data set\n",
    "  file_model : specify file name for saving your model. This way we can upload the model weights from file. We will not to run model again.\n",
    "  \n",
    "\n",
    "  '''\n",
    "  # Create lists to store train and valid loss at each epoch\n",
    "\n",
    "  train_loss_history = []\n",
    "  valid_loss_history = []\n",
    "  #train_acc_history = []\n",
    "  #valid_acc_history = []\n",
    "  delta = 0\n",
    "  best_score = None\n",
    "  valid_loss_min = np.Inf\n",
    "  counter_early_stop=0\n",
    "  early_stop=False\n",
    "\n",
    "\n",
    "  # Iterate for the given number of epochs\n",
    "  for epoch in range(epochs):\n",
    "    t0 = datetime.now()\n",
    "    # Get train loss and accuracy for one epoch\n",
    "\n",
    "    train_loss = train(train_loader, model, optimizer, loss_function, \n",
    "                                  wandb.config.LOG_BATCH, wandb.config.LOG_INTERVAL,\n",
    "                                  wandb.config.GRAD_CLIPPING, wandb.config.MAX_NORM)\n",
    "    valid_loss = valid(valid_loader, model, optimizer, loss_function,\n",
    "                                    wandb.config.LOG_BATCH, wandb.config.LOG_INTERVAL)\n",
    "\n",
    "    dt = datetime.now() - t0\n",
    "\n",
    "    # Save history of the Losses and accuracy\n",
    "    train_loss_history.append(train_loss)\n",
    "    #train_acc_history.append(train_acc)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "    #valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if early_stopping:\n",
    "      score = -valid_loss\n",
    "      if best_score is None:\n",
    "        best_score=score\n",
    "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "      elif score < best_score + delta:\n",
    "        counter_early_stop += 1\n",
    "        print(f'Early stoping counter: {counter_early_stop} out of {patience}')\n",
    "        if counter_early_stop > patience:\n",
    "          early_stop = True\n",
    "\n",
    "      \n",
    "      else:\n",
    "        best_score = score\n",
    "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "        counter_early_stop=0\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "      if early_stop:\n",
    "        print('Early Stopping')\n",
    "        break\n",
    "\n",
    "    elif save_best_model:\n",
    "\n",
    "      score = -valid_loss\n",
    "      if best_score is None:\n",
    "        best_score=score\n",
    "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "      elif score < best_score + delta:\n",
    "        print(f'Validation loss has not decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Not Saving Model...')\n",
    "      \n",
    "      else:\n",
    "        best_score = score\n",
    "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    else:\n",
    "        torch.save(model.state_dict(), file_model)\n",
    "\n",
    "    # Log the train and valid loss to W&B\n",
    "    wandb.log({f\"Train epoch Loss :\": train_loss, f\"Valid epoch Loss :\": valid_loss })\n",
    "    #wandb.log({f\"Train epoch Acc :\": train_acc, f\"Valid epoch Acc :\": valid_acc})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Print the train loss and accuracy for given number of epochs, batch size and number of samples\n",
    "    print(f'Epoch : {epoch+1} / {epochs}')\n",
    "    print(f'Time to complete {epoch+1} is {dt}')\n",
    "    print(f'Learning rate: {scheduler._last_lr[0]}')\n",
    "    \n",
    "    #print(f'Train Loss: {train_loss : .4f} | Train Accuracy: {train_acc * 100 : .4f}%')\n",
    "    #print(f'Valid Loss: {valid_loss : .4f} | Valid Accuracy: {valid_acc * 100 : .4f}%')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss : .4f}')\n",
    "    print(f'Valid Loss: {valid_loss : .4f}')\n",
    "    print()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  #return train_loss_history, train_acc_history, valid_loss_history, valid_acc_history\n",
    "  return train_loss_history, valid_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2hLhgL5-tQE"
   },
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    \n",
    "    HIDDEN_SIZES_LIST = [300] + [200],  # 300 = embed_dim \n",
    "    DPROBS_LIST = [0.0] + [0] ,\n",
    "    BATCHNORM_BINARY = False,\n",
    "    TASK = 5,\n",
    "    \n",
    "    VOCAB_SIZE = len(trainset_vocab),\n",
    "    OUTPUT_DIM = 10,\n",
    "\n",
    "    EPOCHS = 100, \n",
    "    \n",
    "    BATCH_SIZE = 256, \n",
    "    LEARNING_RATE = 0.02, \n",
    "    DATASET=\"StackOverFlow\",\n",
    "    ARCHITECTURE=\"embedding_layer_only\", \n",
    "    LOG_INTERVAL = 25,\n",
    "    LOG_BATCH = True,\n",
    "    FILE_MODEL = model_path/'overfit_exp2.pt',\n",
    "    GRAD_CLIPPING = False,  \n",
    "    MAX_NORM = 0, \n",
    "    MOMENTUM = 0, \n",
    "    PATIENCE = 10,\n",
    "    EARLY_STOPPING = True,\n",
    "    SAVE_BEST_MODEL = False,\n",
    "    SCHEDULER_FACTOR = 0.8,\n",
    "    SCHEDULER_PATIENCE = 0,\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "   )\n",
    "\n",
    "non_linearity = F.selu\n",
    "pretrained_weights_tensor = torch.tensor(pretrained_weights).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "executionInfo": {
     "elapsed": 8303,
     "status": "ok",
     "timestamp": 1651387220786,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "-UTKKuZB_LQs",
    "outputId": "83d714a8-0341-4b5d-ad6e-55cc175e3f4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvikashk\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20220501_064010-u7eh9xi9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vikashk/hw6_part_b_full_task6/runs/u7eh9xi9\" target=\"_blank\">exp_1</a></strong> to <a href=\"https://wandb.ai/vikashk/hw6_part_b_full_task6\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/vikashk/hw6_part_b_full_task6/runs/u7eh9xi9?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5340da8d90>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(name = 'exp_1', project = 'hw6_part_b_full_task6', config = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBYcJ5Ec_QRw"
   },
   "outputs": [],
   "source": [
    "wandb.config.NON_LINEARITY = non_linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6xFxr0W_TzV"
   },
   "outputs": [],
   "source": [
    "# Fix seed value\n",
    "from datetime import datetime\n",
    "SEED = 2345\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=wandb.config.BATCH_SIZE, shuffle =True, \n",
    "                                           collate_fn=collate_batch, num_workers = 4)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=wandb.config.BATCH_SIZE, shuffle = False, \n",
    "                                           collate_fn=collate_batch,  num_workers = 4)\n",
    "                                         \n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=wandb.config.BATCH_SIZE, shuffle = False, \n",
    "                                           collate_fn=collate_batch,  num_workers = 4)\n",
    "\n",
    "# cross entropy loss function\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# use GPUs\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model \n",
    "                \n",
    "model_hw6_part_b_task6  = MLPCustom(wandb.config.VOCAB_SIZE, \n",
    "                                    wandb.config.HIDDEN_SIZES_LIST, \n",
    "                                    wandb.config.DPROBS_LIST, \n",
    "                                    wandb.config.BATCHNORM_BINARY, \n",
    "                                    wandb.config.OUTPUT_DIM, \n",
    "                                    non_linearity, pretrained_weights_tensor)\n",
    "\n",
    "model_hw6_part_b_task6.to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "  if type(m) == nn.Linear:\n",
    "      #torch.nn.init.normal_(m.weight, mean = 0, std = 1)\n",
    "      torch.nn.init.kaiming_normal_(m.weight)\n",
    "      torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "  #if type(m) == nn.EmbeddingBag:\n",
    "      #torch.nn.init.normal_(m.weight, mean = 0, std = 1)\n",
    "        \n",
    "# apply initialization recursively  to all modules\n",
    "# model_hw6_part_a.apply(init_weights)\n",
    "\n",
    "wandb.config.initialization = 'Default'\n",
    "\n",
    "# Intialize stochiastic gradient descent optimizer\n",
    "#optimizer = torch.optim.SGD(model_imdb.parameters(), lr = wandb.config.LEARNING_RATE, \n",
    "#                            weight_decay=wandb.config.WEIGHT_DECAY, momentum = wandb.config.MOMENTUM)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_hw6_part_b_task6.parameters(), lr = wandb.config.LEARNING_RATE, \n",
    "                            weight_decay=wandb.config.WEIGHT_DECAY)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor= wandb.config.SCHEDULER_FACTOR, \n",
    "                              patience=wandb.config.SCHEDULER_PATIENCE, verbose=True)\n",
    "\n",
    "# scheduler = StepLR(optimizer, gamma=0.4,step_size=1, verbose=True)\n",
    "\n",
    "wandb.config.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1615,
     "status": "ok",
     "timestamp": 1651387295067,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "Ta2H-EnXAWZN",
    "outputId": "4b8b7b70-dca6-4f3a-9a21-990979faa78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 0.6929893493652344\n",
      "Expected Theoretical loss: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# Fix seed value\n",
    "\n",
    "SEED = 2345\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "for text , targets, offsets in train_loader:\n",
    "  \n",
    "  # move inputs and outputs to GPUs\n",
    "  text = text.to(device)\n",
    "  targets = targets.to(device)\n",
    "  offsets = offsets.to(device)\n",
    "  \n",
    "  model_hw6_part_b_task6.eval()\n",
    "  # Forward pass\n",
    "  output = model_hw6_part_b_task6(text, offsets)\n",
    "  loss = loss_function(output, targets)\n",
    "  print(f'Actual loss: {loss}')\n",
    "  break\n",
    "\n",
    "print(f'Expected Theoretical loss: {np.log(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1651387295068,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "VZ6aAtVIAvVV",
    "outputId": "451edf34-004a-4d31-ecab-c638294f58e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f5340e40710>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model_hw6_part_b_task6, log = 'all', log_freq=25, log_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 714898,
     "status": "ok",
     "timestamp": 1651388009960,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "AtPuXU6VA_S2",
    "outputId": "b191fac5-088e-4c5d-e4bd-07a7359ec5c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss has decreased (inf --> 0.183728). Saving Model...\n",
      "Epoch : 1 / 100\n",
      "Time to complete 1 is 0:00:11.072564\n",
      "Learning rate: 0.02\n",
      "Train Loss:  0.2090\n",
      "Valid Loss:  0.1837\n",
      "\n",
      "Validation loss has decreased (0.183728 --> 0.176959). Saving model...\n",
      "Epoch : 2 / 100\n",
      "Time to complete 2 is 0:00:11.184471\n",
      "Learning rate: 0.02\n",
      "Train Loss:  0.1761\n",
      "Valid Loss:  0.1770\n",
      "\n",
      "Validation loss has decreased (0.176959 --> 0.171620). Saving model...\n",
      "Epoch : 3 / 100\n",
      "Time to complete 3 is 0:00:11.535680\n",
      "Learning rate: 0.02\n",
      "Train Loss:  0.1685\n",
      "Valid Loss:  0.1716\n",
      "\n",
      "Validation loss has decreased (0.171620 --> 0.166971). Saving model...\n",
      "Epoch : 4 / 100\n",
      "Time to complete 4 is 0:00:11.350028\n",
      "Learning rate: 0.02\n",
      "Train Loss:  0.1656\n",
      "Valid Loss:  0.1670\n",
      "\n",
      "Validation loss has decreased (0.166971 --> 0.165001). Saving model...\n",
      "Epoch : 5 / 100\n",
      "Time to complete 5 is 0:00:11.498914\n",
      "Learning rate: 0.02\n",
      "Train Loss:  0.1637\n",
      "Valid Loss:  0.1650\n",
      "\n",
      "Validation loss has decreased (0.165001 --> 0.164230). Saving model...\n",
      "Epoch : 6 / 100\n",
      "Time to complete 6 is 0:00:11.484219\n",
      "Learning rate: 0.02\n",
      "Train Loss:  0.1618\n",
      "Valid Loss:  0.1642\n",
      "\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.6000e-02.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 7 / 100\n",
      "Time to complete 7 is 0:00:11.533875\n",
      "Learning rate: 0.016\n",
      "Train Loss:  0.1610\n",
      "Valid Loss:  0.1661\n",
      "\n",
      "Validation loss has decreased (0.164230 --> 0.162329). Saving model...\n",
      "Epoch : 8 / 100\n",
      "Time to complete 8 is 0:00:11.204812\n",
      "Learning rate: 0.016\n",
      "Train Loss:  0.1595\n",
      "Valid Loss:  0.1623\n",
      "\n",
      "Validation loss has decreased (0.162329 --> 0.159796). Saving model...\n",
      "Epoch : 9 / 100\n",
      "Time to complete 9 is 0:00:11.545014\n",
      "Learning rate: 0.016\n",
      "Train Loss:  0.1581\n",
      "Valid Loss:  0.1598\n",
      "\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.2800e-02.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 10 / 100\n",
      "Time to complete 10 is 0:00:11.324510\n",
      "Learning rate: 0.0128\n",
      "Train Loss:  0.1585\n",
      "Valid Loss:  0.1613\n",
      "\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0240e-02.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 11 / 100\n",
      "Time to complete 11 is 0:00:11.340430\n",
      "Learning rate: 0.01024\n",
      "Train Loss:  0.1566\n",
      "Valid Loss:  0.1602\n",
      "\n",
      "Epoch 00012: reducing learning rate of group 0 to 8.1920e-03.\n",
      "Early stoping counter: 3 out of 10\n",
      "Epoch : 12 / 100\n",
      "Time to complete 12 is 0:00:11.279730\n",
      "Learning rate: 0.008192000000000001\n",
      "Train Loss:  0.1555\n",
      "Valid Loss:  0.1617\n",
      "\n",
      "Validation loss has decreased (0.159796 --> 0.158607). Saving model...\n",
      "Epoch : 13 / 100\n",
      "Time to complete 13 is 0:00:11.299085\n",
      "Learning rate: 0.008192000000000001\n",
      "Train Loss:  0.1543\n",
      "Valid Loss:  0.1586\n",
      "\n",
      "Validation loss has decreased (0.158607 --> 0.158271). Saving model...\n",
      "Epoch : 14 / 100\n",
      "Time to complete 14 is 0:00:11.418289\n",
      "Learning rate: 0.008192000000000001\n",
      "Train Loss:  0.1541\n",
      "Valid Loss:  0.1583\n",
      "\n",
      "Epoch 00015: reducing learning rate of group 0 to 6.5536e-03.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 15 / 100\n",
      "Time to complete 15 is 0:00:11.512346\n",
      "Learning rate: 0.0065536000000000014\n",
      "Train Loss:  0.1532\n",
      "Valid Loss:  0.1585\n",
      "\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.2429e-03.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 16 / 100\n",
      "Time to complete 16 is 0:00:11.258600\n",
      "Learning rate: 0.005242880000000002\n",
      "Train Loss:  0.1525\n",
      "Valid Loss:  0.1584\n",
      "\n",
      "Validation loss has decreased (0.158271 --> 0.157464). Saving model...\n",
      "Epoch : 17 / 100\n",
      "Time to complete 17 is 0:00:11.296874\n",
      "Learning rate: 0.005242880000000002\n",
      "Train Loss:  0.1514\n",
      "Valid Loss:  0.1575\n",
      "\n",
      "Validation loss has decreased (0.157464 --> 0.157412). Saving model...\n",
      "Epoch : 18 / 100\n",
      "Time to complete 18 is 0:00:11.775126\n",
      "Learning rate: 0.005242880000000002\n",
      "Train Loss:  0.1516\n",
      "Valid Loss:  0.1574\n",
      "\n",
      "Validation loss has decreased (0.157412 --> 0.157041). Saving model...\n",
      "Epoch : 19 / 100\n",
      "Time to complete 19 is 0:00:11.452550\n",
      "Learning rate: 0.005242880000000002\n",
      "Train Loss:  0.1512\n",
      "Valid Loss:  0.1570\n",
      "\n",
      "Validation loss has decreased (0.157041 --> 0.155446). Saving model...\n",
      "Epoch : 20 / 100\n",
      "Time to complete 20 is 0:00:11.490399\n",
      "Learning rate: 0.005242880000000002\n",
      "Train Loss:  0.1516\n",
      "Valid Loss:  0.1554\n",
      "\n",
      "Epoch 00021: reducing learning rate of group 0 to 4.1943e-03.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 21 / 100\n",
      "Time to complete 21 is 0:00:11.467079\n",
      "Learning rate: 0.004194304000000002\n",
      "Train Loss:  0.1510\n",
      "Valid Loss:  0.1563\n",
      "\n",
      "Epoch 00022: reducing learning rate of group 0 to 3.3554e-03.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 22 / 100\n",
      "Time to complete 22 is 0:00:11.420390\n",
      "Learning rate: 0.003355443200000002\n",
      "Train Loss:  0.1507\n",
      "Valid Loss:  0.1570\n",
      "\n",
      "Epoch 00023: reducing learning rate of group 0 to 2.6844e-03.\n",
      "Early stoping counter: 3 out of 10\n",
      "Epoch : 23 / 100\n",
      "Time to complete 23 is 0:00:11.252666\n",
      "Learning rate: 0.0026843545600000016\n",
      "Train Loss:  0.1498\n",
      "Valid Loss:  0.1570\n",
      "\n",
      "Epoch 00024: reducing learning rate of group 0 to 2.1475e-03.\n",
      "Early stoping counter: 4 out of 10\n",
      "Epoch : 24 / 100\n",
      "Time to complete 24 is 0:00:11.369134\n",
      "Learning rate: 0.0021474836480000013\n",
      "Train Loss:  0.1494\n",
      "Valid Loss:  0.1556\n",
      "\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.7180e-03.\n",
      "Early stoping counter: 5 out of 10\n",
      "Epoch : 25 / 100\n",
      "Time to complete 25 is 0:00:11.364177\n",
      "Learning rate: 0.0017179869184000011\n",
      "Train Loss:  0.1488\n",
      "Valid Loss:  0.1555\n",
      "\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.3744e-03.\n",
      "Early stoping counter: 6 out of 10\n",
      "Epoch : 26 / 100\n",
      "Time to complete 26 is 0:00:11.368687\n",
      "Learning rate: 0.001374389534720001\n",
      "Train Loss:  0.1483\n",
      "Valid Loss:  0.1558\n",
      "\n",
      "Validation loss has decreased (0.155446 --> 0.155221). Saving model...\n",
      "Epoch : 27 / 100\n",
      "Time to complete 27 is 0:00:11.244500\n",
      "Learning rate: 0.001374389534720001\n",
      "Train Loss:  0.1478\n",
      "Valid Loss:  0.1552\n",
      "\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0995e-03.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 28 / 100\n",
      "Time to complete 28 is 0:00:11.572448\n",
      "Learning rate: 0.001099511627776001\n",
      "Train Loss:  0.1473\n",
      "Valid Loss:  0.1554\n",
      "\n",
      "Validation loss has decreased (0.155221 --> 0.155066). Saving model...\n",
      "Epoch : 29 / 100\n",
      "Time to complete 29 is 0:00:11.324541\n",
      "Learning rate: 0.001099511627776001\n",
      "Train Loss:  0.1472\n",
      "Valid Loss:  0.1551\n",
      "\n",
      "Validation loss has decreased (0.155066 --> 0.154975). Saving model...\n",
      "Epoch : 30 / 100\n",
      "Time to complete 30 is 0:00:11.519996\n",
      "Learning rate: 0.001099511627776001\n",
      "Train Loss:  0.1468\n",
      "Valid Loss:  0.1550\n",
      "\n",
      "Epoch 00031: reducing learning rate of group 0 to 8.7961e-04.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 31 / 100\n",
      "Time to complete 31 is 0:00:11.314279\n",
      "Learning rate: 0.0008796093022208008\n",
      "Train Loss:  0.1472\n",
      "Valid Loss:  0.1551\n",
      "\n",
      "Epoch 00032: reducing learning rate of group 0 to 7.0369e-04.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 32 / 100\n",
      "Time to complete 32 is 0:00:11.324977\n",
      "Learning rate: 0.0007036874417766407\n",
      "Train Loss:  0.1469\n",
      "Valid Loss:  0.1552\n",
      "\n",
      "Validation loss has decreased (0.154975 --> 0.154882). Saving model...\n",
      "Epoch : 33 / 100\n",
      "Time to complete 33 is 0:00:11.295839\n",
      "Learning rate: 0.0007036874417766407\n",
      "Train Loss:  0.1468\n",
      "Valid Loss:  0.1549\n",
      "\n",
      "Epoch 00034: reducing learning rate of group 0 to 5.6295e-04.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 34 / 100\n",
      "Time to complete 34 is 0:00:11.463278\n",
      "Learning rate: 0.0005629499534213126\n",
      "Train Loss:  0.1463\n",
      "Valid Loss:  0.1549\n",
      "\n",
      "Validation loss has decreased (0.154882 --> 0.154767). Saving model...\n",
      "Epoch : 35 / 100\n",
      "Time to complete 35 is 0:00:11.378641\n",
      "Learning rate: 0.0005629499534213126\n",
      "Train Loss:  0.1463\n",
      "Valid Loss:  0.1548\n",
      "\n",
      "Epoch 00036: reducing learning rate of group 0 to 4.5036e-04.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 36 / 100\n",
      "Time to complete 36 is 0:00:11.366364\n",
      "Learning rate: 0.0004503599627370501\n",
      "Train Loss:  0.1465\n",
      "Valid Loss:  0.1548\n",
      "\n",
      "Epoch 00037: reducing learning rate of group 0 to 3.6029e-04.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 37 / 100\n",
      "Time to complete 37 is 0:00:11.280749\n",
      "Learning rate: 0.0003602879701896401\n",
      "Train Loss:  0.1468\n",
      "Valid Loss:  0.1548\n",
      "\n",
      "Validation loss has decreased (0.154767 --> 0.154729). Saving model...\n",
      "Epoch : 38 / 100\n",
      "Time to complete 38 is 0:00:11.293304\n",
      "Learning rate: 0.0003602879701896401\n",
      "Train Loss:  0.1464\n",
      "Valid Loss:  0.1547\n",
      "\n",
      "Validation loss has decreased (0.154729 --> 0.154710). Saving model...\n",
      "Epoch : 39 / 100\n",
      "Time to complete 39 is 0:00:11.273725\n",
      "Learning rate: 0.0003602879701896401\n",
      "Train Loss:  0.1462\n",
      "Valid Loss:  0.1547\n",
      "\n",
      "Validation loss has decreased (0.154710 --> 0.154645). Saving model...\n",
      "Epoch : 40 / 100\n",
      "Time to complete 40 is 0:00:11.470856\n",
      "Learning rate: 0.0003602879701896401\n",
      "Train Loss:  0.1464\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.8823e-04.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 41 / 100\n",
      "Time to complete 41 is 0:00:11.463498\n",
      "Learning rate: 0.0002882303761517121\n",
      "Train Loss:  0.1459\n",
      "Valid Loss:  0.1547\n",
      "\n",
      "Epoch 00042: reducing learning rate of group 0 to 2.3058e-04.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 42 / 100\n",
      "Time to complete 42 is 0:00:11.291950\n",
      "Learning rate: 0.00023058430092136968\n",
      "Train Loss:  0.1462\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Validation loss has decreased (0.154645 --> 0.154604). Saving model...\n",
      "Epoch : 43 / 100\n",
      "Time to complete 43 is 0:00:11.268065\n",
      "Learning rate: 0.00023058430092136968\n",
      "Train Loss:  0.1457\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Validation loss has decreased (0.154604 --> 0.154555). Saving model...\n",
      "Epoch : 44 / 100\n",
      "Time to complete 44 is 0:00:11.475933\n",
      "Learning rate: 0.00023058430092136968\n",
      "Train Loss:  0.1456\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.8447e-04.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 45 / 100\n",
      "Time to complete 45 is 0:00:11.467727\n",
      "Learning rate: 0.00018446744073709575\n",
      "Train Loss:  0.1457\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.4757e-04.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 46 / 100\n",
      "Time to complete 46 is 0:00:11.397957\n",
      "Learning rate: 0.00014757395258967662\n",
      "Train Loss:  0.1456\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.1806e-04.\n",
      "Validation loss has decreased (0.154555 --> 0.154550). Saving model...\n",
      "Epoch : 47 / 100\n",
      "Time to complete 47 is 0:00:11.255879\n",
      "Learning rate: 0.0001180591620717413\n",
      "Train Loss:  0.1453\n",
      "Valid Loss:  0.1545\n",
      "\n",
      "Epoch 00048: reducing learning rate of group 0 to 9.4447e-05.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 48 / 100\n",
      "Time to complete 48 is 0:00:11.450258\n",
      "Learning rate: 9.444732965739304e-05\n",
      "Train Loss:  0.1454\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00049: reducing learning rate of group 0 to 7.5558e-05.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 49 / 100\n",
      "Time to complete 49 is 0:00:11.297342\n",
      "Learning rate: 7.555786372591443e-05\n",
      "Train Loss:  0.1457\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00050: reducing learning rate of group 0 to 6.0446e-05.\n",
      "Early stoping counter: 3 out of 10\n",
      "Epoch : 50 / 100\n",
      "Time to complete 50 is 0:00:11.284902\n",
      "Learning rate: 6.044629098073155e-05\n",
      "Train Loss:  0.1460\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00051: reducing learning rate of group 0 to 4.8357e-05.\n",
      "Validation loss has decreased (0.154550 --> 0.154545). Saving model...\n",
      "Epoch : 51 / 100\n",
      "Time to complete 51 is 0:00:11.332311\n",
      "Learning rate: 4.8357032784585246e-05\n",
      "Train Loss:  0.1458\n",
      "Valid Loss:  0.1545\n",
      "\n",
      "Epoch 00052: reducing learning rate of group 0 to 3.8686e-05.\n",
      "Early stoping counter: 1 out of 10\n",
      "Epoch : 52 / 100\n",
      "Time to complete 52 is 0:00:11.200196\n",
      "Learning rate: 3.86856262276682e-05\n",
      "Train Loss:  0.1451\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00053: reducing learning rate of group 0 to 3.0949e-05.\n",
      "Early stoping counter: 2 out of 10\n",
      "Epoch : 53 / 100\n",
      "Time to complete 53 is 0:00:11.244555\n",
      "Learning rate: 3.094850098213456e-05\n",
      "Train Loss:  0.1455\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00054: reducing learning rate of group 0 to 2.4759e-05.\n",
      "Early stoping counter: 3 out of 10\n",
      "Epoch : 54 / 100\n",
      "Time to complete 54 is 0:00:11.240034\n",
      "Learning rate: 2.4758800785707648e-05\n",
      "Train Loss:  0.1456\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.9807e-05.\n",
      "Early stoping counter: 4 out of 10\n",
      "Epoch : 55 / 100\n",
      "Time to complete 55 is 0:00:11.334418\n",
      "Learning rate: 1.980704062856612e-05\n",
      "Train Loss:  0.1452\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.5846e-05.\n",
      "Early stoping counter: 5 out of 10\n",
      "Epoch : 56 / 100\n",
      "Time to complete 56 is 0:00:11.496833\n",
      "Learning rate: 1.5845632502852897e-05\n",
      "Train Loss:  0.1455\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.2677e-05.\n",
      "Early stoping counter: 6 out of 10\n",
      "Epoch : 57 / 100\n",
      "Time to complete 57 is 0:00:11.339486\n",
      "Learning rate: 1.2676506002282318e-05\n",
      "Train Loss:  0.1454\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0141e-05.\n",
      "Early stoping counter: 7 out of 10\n",
      "Epoch : 58 / 100\n",
      "Time to complete 58 is 0:00:11.491665\n",
      "Learning rate: 1.0141204801825855e-05\n",
      "Train Loss:  0.1455\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00059: reducing learning rate of group 0 to 8.1130e-06.\n",
      "Early stoping counter: 8 out of 10\n",
      "Epoch : 59 / 100\n",
      "Time to complete 59 is 0:00:11.538498\n",
      "Learning rate: 8.112963841460684e-06\n",
      "Train Loss:  0.1455\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00060: reducing learning rate of group 0 to 6.4904e-06.\n",
      "Early stoping counter: 9 out of 10\n",
      "Epoch : 60 / 100\n",
      "Time to complete 60 is 0:00:11.199888\n",
      "Learning rate: 6.490371073168548e-06\n",
      "Train Loss:  0.1453\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00061: reducing learning rate of group 0 to 5.1923e-06.\n",
      "Early stoping counter: 10 out of 10\n",
      "Epoch : 61 / 100\n",
      "Time to complete 61 is 0:00:11.235633\n",
      "Learning rate: 5.192296858534838e-06\n",
      "Train Loss:  0.1453\n",
      "Valid Loss:  0.1546\n",
      "\n",
      "Epoch 00062: reducing learning rate of group 0 to 4.1538e-06.\n",
      "Early stoping counter: 11 out of 10\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "example_ct_train, batch_ct_train, example_ct_valid, batch_ct_valid = 0, 0, 0, 0\n",
    "train_loss_history, valid_loss_history = train_loop(train_loader,\n",
    "                                                    valid_loader, \n",
    "                                                    model_hw6_part_b_task6, \n",
    "                                                    loss_function, \n",
    "                                                    optimizer, \n",
    "                                                    wandb.config.EPOCHS, \n",
    "                                                    device,\n",
    "                                                    wandb.config.PATIENCE, \n",
    "                                                    wandb.config.EARLY_STOPPING,\n",
    "                                                    wandb.config.FILE_MODEL, \n",
    "                                                    wandb.config.SAVE_BEST_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1g-619j3CyQE"
   },
   "outputs": [],
   "source": [
    "def get_pred(data_loader, model):\n",
    "  \"\"\" \n",
    "  Function to get predictions for a given test set and calculate accuracy.\n",
    "  Input: Iterator to the test set.\n",
    "  Output: Prections and Accuracy for test set.\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    # Array to store predicted labels\n",
    "    predictions = torch.Tensor()\n",
    "    predictions = predictions.to(device)\n",
    "    \n",
    "    outputs = torch.Tensor()\n",
    "    outputs = outputs.to(device)\n",
    "\n",
    "    # Array to store actual labels\n",
    "    y = torch.Tensor()\n",
    "    y = y.to(device)\n",
    "    # Iterate over batches from test set\n",
    "    for text, targets, offsets in data_loader:\n",
    "      \n",
    "      # move inputs and outputs to GPUs\n",
    "      text = text.to(device)\n",
    "      targets = targets.to(device)\n",
    "      offsets = offsets.to(device)\n",
    "      \n",
    "      # Calculated the predicted labels\n",
    "      output = model(text, offsets)\n",
    "      predicted_y = output.clone()\n",
    "\n",
    "      # Update teh output \n",
    "      predicted_y[predicted_y>0] = 1\n",
    "      predicted_y[predicted_y<=0] =0\n",
    "\n",
    "      # Add the predicted labels to the array\n",
    "      predictions = torch.cat((predictions, predicted_y)) \n",
    "    \n",
    "      outputs = torch.cat((outputs, output)) \n",
    "\n",
    "      # Add the actual labels to the array\n",
    "      y = torch.cat((y, targets)) \n",
    "\n",
    "  # Return array containing predictions and accuracy\n",
    "  return y, predictions\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1651388010091,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "--34m2D-Ji2B",
    "outputId": "469af819-e405-441f-f10d-0e7ccc39da83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn =MLPCustom(wandb.config.VOCAB_SIZE, \n",
    "                                    wandb.config.HIDDEN_SIZES_LIST, \n",
    "                                    wandb.config.DPROBS_LIST, \n",
    "                                    wandb.config.BATCHNORM_BINARY, \n",
    "                                    wandb.config.OUTPUT_DIM, \n",
    "                                    non_linearity, pretrained_weights_tensor)\n",
    "model_nn.to(device)\n",
    "model_nn.load_state_dict(torch.load(wandb.config.FILE_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1651388010202,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "_pyBYrRHKFkJ",
    "outputId": "b2643803-4db6-4f38-9882-c63a719ee029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Datasets_Models/Models/overfit_exp2.pt\n"
     ]
    }
   ],
   "source": [
    "print(wandb.config.FILE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cnkM2LKKHyJ"
   },
   "outputs": [],
   "source": [
    "# Get the prediction and labels\n",
    "y_train,  y_predicted_train = get_pred(train_loader, model_nn)\n",
    "y_valid, y_predicted_valid = get_pred(valid_loader, model_nn)\n",
    "y_test,  y_predicted_test  = get_pred(test_loader, model_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6480,
     "status": "ok",
     "timestamp": 1651388026108,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "pqE8yUbJKLSW",
    "outputId": "228baecb-c3e0-4ff8-f84f-d51acee736e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▉                               | 10 kB 20.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 20 kB 20.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 30 kB 11.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 40 kB 8.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 51 kB 4.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 61 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 71 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 81 kB 4.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 92 kB 4.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 102 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 112 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 122 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 133 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 143 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 153 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 163 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 174 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 184 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 194 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 204 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 215 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 225 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 235 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 245 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 256 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 266 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 276 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 286 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 296 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 307 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 317 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 327 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 337 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 348 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 358 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 368 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 378 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 389 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 399 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 408 kB 5.3 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics -U -qq\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGwVL1vGKVNG"
   },
   "outputs": [],
   "source": [
    "f1score  = F1Score(num_classes=10, mdmc_average= 'global').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeThQZyfKYHc"
   },
   "outputs": [],
   "source": [
    "train_f1_score = f1score( y_predicted_train, y_train.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1651388026193,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "AkCf0Wv8KZdX",
    "outputId": "50450591-7a53-40b3-ded9-d8c8e73520b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8756, device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jan3fPbbKamA"
   },
   "outputs": [],
   "source": [
    "# convert these to numpy array\n",
    "y_train, y_predicted_train  = y_train.cpu().numpy(), y_predicted_train.cpu().numpy() \n",
    "y_valid, y_predicted_valid  = y_valid.cpu().numpy(), y_predicted_valid.cpu().numpy() \n",
    "y_test, y_predicted_test  = y_test.cpu().numpy(), y_predicted_test.cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85pY-MXDKb4S"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81yQPpuOKdEO"
   },
   "outputs": [],
   "source": [
    "f1_score_train = f1_score(y_train, y_predicted_train, average = 'micro')\n",
    "f1_score_valid = f1_score(y_valid, y_predicted_valid, average = 'micro')\n",
    "f1_score_test = f1_score(y_test, y_predicted_test, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1651388026530,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "p3aOohc7KeH8",
    "outputId": "a9c249f4-7fe2-41da-815e-2b3df9a2352a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score_train 0.8755512133294605\n",
      "f1_score_valid 0.8653519932145888\n",
      "f1_score_test 0.8617842876165113\n"
     ]
    }
   ],
   "source": [
    "# Print Accuracy based on saved Model\n",
    "print('f1_score_train', f1_score_train)\n",
    "print('f1_score_valid', f1_score_valid)\n",
    "print('f1_score_test', f1_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7ktkaMLKfQg"
   },
   "outputs": [],
   "source": [
    "wandb.log({'Train f1 score': f1_score_train})\n",
    "wandb.log({'Valid f1 score': f1_score_valid}) \n",
    "wandb.log({'Test f1 score': f1_score_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330,
     "referenced_widgets": [
      "2b96a0820e294447a65fd61434341f4c",
      "7b07c471cc9b460fa7423f7e14262c39",
      "7dd35087d3774f78859221af76b1c1db",
      "a4e80e39c0484213a48863640c34fde1",
      "14ad72acb98e49338e0d9b9edfda545a",
      "02f6729fb5e34d8b9f8aab911fd1994c",
      "d4e55a37700344c1a900c64a2c711db4",
      "17d83fa7537a44a5b77641159df67d4b"
     ]
    },
    "executionInfo": {
     "elapsed": 6639,
     "status": "ok",
     "timestamp": 1651388033166,
     "user": {
      "displayName": "Vikash Kumar",
      "userId": "09277849211533188513"
     },
     "user_tz": 300
    },
    "id": "oRLh5zCmKgiy",
    "outputId": "3fe54ace-7db5-4fb7-ced6-413e73e7b82e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b96a0820e294447a65fd61434341f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test f1 score</td><td>▁</td></tr><tr><td>Train Batch Loss  :</td><td>▆▆▆▃▆▅█▄▆▄▅▅▃▆▃▅▇▃▄▅▄▆▃▂▃▄▂▃▃▂▃▃▃▃▆▄▃▁▄▄</td></tr><tr><td>Train epoch Loss :</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train f1 score</td><td>▁</td></tr><tr><td>Valid Batch Loss  :</td><td>▆▄█▅▄▄▄█▁▄▄▄▄▄▅▅▃▂▆▅▃▅▃█▁▄▅▄▄▄▅▅▃▂▂▅▃▅▃▄</td></tr><tr><td>Valid epoch Loss :</td><td>█▆▄▄▄▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Valid f1 score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test f1 score</td><td>0.86178</td></tr><tr><td>Train Batch Loss  :</td><td>0.13024</td></tr><tr><td>Train epoch Loss :</td><td>0.14532</td></tr><tr><td>Train f1 score</td><td>0.87555</td></tr><tr><td>Valid Batch Loss  :</td><td>0.14885</td></tr><tr><td>Valid epoch Loss :</td><td>0.15455</td></tr><tr><td>Valid f1 score</td><td>0.86535</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">exp_1</strong>: <a href=\"https://wandb.ai/vikashk/hw6_part_b_full_task6/runs/u7eh9xi9\" target=\"_blank\">https://wandb.ai/vikashk/hw6_part_b_full_task6/runs/u7eh9xi9</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220501_064010-u7eh9xi9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCgu-XLhKhjc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNw4Uxn2xUW1/J5NRWKCpX0",
   "collapsed_sections": [],
   "name": "Stackoverflow_Multilabel_Tagging_FineTune_PreTrained_Embedding_Neural_Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02f6729fb5e34d8b9f8aab911fd1994c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14ad72acb98e49338e0d9b9edfda545a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17d83fa7537a44a5b77641159df67d4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b96a0820e294447a65fd61434341f4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b07c471cc9b460fa7423f7e14262c39",
       "IPY_MODEL_7dd35087d3774f78859221af76b1c1db"
      ],
      "layout": "IPY_MODEL_a4e80e39c0484213a48863640c34fde1"
     }
    },
    "7b07c471cc9b460fa7423f7e14262c39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14ad72acb98e49338e0d9b9edfda545a",
      "placeholder": "​",
      "style": "IPY_MODEL_02f6729fb5e34d8b9f8aab911fd1994c",
      "value": "0.042 MB of 0.042 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "7dd35087d3774f78859221af76b1c1db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4e55a37700344c1a900c64a2c711db4",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17d83fa7537a44a5b77641159df67d4b",
      "value": 1
     }
    },
    "a4e80e39c0484213a48863640c34fde1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4e55a37700344c1a900c64a2c711db4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
